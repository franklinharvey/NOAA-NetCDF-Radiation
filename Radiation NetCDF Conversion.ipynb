{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Radiation's \".dat\" files to a NetCDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files found in radiations FTP server (found at ftp://ftp.cmdl.noaa.gov in /g-rad/baseline/) are organized by test sites like ALT (Alert Observatory in Alert, Nunavut, Canada; a BSRN site) and BRW (Barrow Observatory in Barrow, Alaska, United States; a baseline GMD observatory). There are six total baseline observatories and four BSRN sites. Each testing location is a directory holding a zip files, one per month. Each zip file, when unzipped, contains a single \".dat\" file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    ALT_RAD                                                \n",
    "                                                   ALT_RAD2                \n",
    "                     DIRECT        D_GLOBAL        U_GLOBAL          Zenith\n",
    "   Year Mn Dy Hr Mi        DIFFUSE2            D_IR            U_IR        \n",
    "   2004  9  1  0  1    1.04   79.40   78.67  303.58   61.06  310.95  85.142\n",
    "   2004  9  1  0  2    0.71   74.36   73.91  303.80   57.82  310.92  85.171\n",
    "   2004  9  1  0  3    0.67   71.80   71.64  304.25   56.84  310.98  85.199\n",
    "   2004  9  1  0  4    0.75   74.35   74.83  304.21   59.68  310.89  85.227"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first eight lines of the first file in the ALT directory (\"alt_2004_09.dat\"). The first issue is evident right away: the headers are not all on one line. In fact, some headers use two lines. This format is fairly legible to humans, but any program would have a hard time reading this. I decided the first thing to do would be to convert these files to CSV's (as most data libraries would certainly have readers for CSV's). There is an irregular amount of whitespace between columns, but I knew python wouldn't have had an issue with this.\n",
    "\n",
    "My second issue is exemplified with these exerpts from the BAO directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                   BAO0_RAD                                                \n",
    "                                                   BAO8_RAD                \n",
    "                     DIRECT        D_GLOBAL        U_GLOBAL          Zenith\n",
    "   Year Mn Dy Hr Mi         DIFFUSE            D_IR            U_IR        \n",
    "   1992  1  1  0  3 -999.00 -999.00 -999.00 -999.00 -999.00 -999.00  93.734\n",
    "   1992  1  1  0  6 -999.00 -999.00 -999.00 -999.00 -999.00 -999.00  94.245\n",
    "   1992  1  1  0  9 -999.00 -999.00 -999.00 -999.00 -999.00 -999.00  94.758\n",
    "   1992  1  1  0 12 -999.00 -999.00 -999.00 -999.00 -999.00 -999.00  95.273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    BAO_RAD                                \n",
    "                                                           \n",
    "                     DIRECT        D_GLOBAL          Zenith\n",
    "   Year Mn Dy Hr Mi        DIFFUSE2            D_IR        \n",
    "   2016  5  1  0  1    0.32  105.18  105.66  302.53  69.491\n",
    "   2016  5  1  0  2    0.37   97.06   96.79  306.56  69.681\n",
    "   2016  5  1  0  3    0.24   91.87   92.35  307.61  69.872\n",
    "   2016  5  1  0  4    0.00   93.20   94.02  306.54  70.062"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files shown are \"bao_1992_01.dat\" (shown first) and \"bao_2016_05.dat\" (shown second). The headers change. Specifically, \"DIFFUSE\" changes to \"DIFFUSE2\". Additionally \"BAO8_RAD U_GLOBAL\" and \"U_IR\" are removed. The headers swap at the beginning of 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to convert \".dat\" to \".csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will take these steps: import file, parse headers, set flags for each possible header, write present headers, write data delimited by commas. Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv #to write to a csv\n",
    "from os.path import basename\n",
    "import os #to get filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = \"../baselineRad/bao/dat/bao_1992_01.dat\" #this is formatted for my directory layout, and normally would use a queue for all files in a directory\n",
    "Direct, Diffuse, Diffuse2, D_Global, D_IR, U_Global, U_Global2, U_IR, Zenith, check= False, False, False, False, False, False, False, False, False, False\n",
    "#set flags for all headers and a checking variable to false, these will be set to true as they are found\n",
    "\n",
    "\n",
    "base = os.path.splitext(basename(input))[0] #get name of input file \"bao_1992_01\" in this case\n",
    "baseFolder = base.split('_',1)[0] + \"/csv/\" #returns \"bao/csv/\"\n",
    "out_name = (\"../baselineRad/\" + baseFolder + base + \".csv\") #sets output to \"../baselineRad/bao/csv/bao_1992_01.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeHeaders(output_file, Direct, Diffuse, Diffuse2, D_Global, D_IR, U_Global, U_Global2, U_IR, Zenith):\n",
    "    output_file.write(\"Year,Month,Day,Hour,Minute,\")\n",
    "    if Direct:\n",
    "        output_file.write(\"DIRECT,\")\n",
    "    if Diffuse:\n",
    "        output_file.write(\"DIFFUSE,\")\n",
    "    if Diffuse2:\n",
    "        output_file.write(\"DIFFUSE2,\")\n",
    "    if D_Global:\n",
    "        output_file.write(\"D_GLOBAL,\")\n",
    "    if D_IR:\n",
    "        output_file.write(\"D_IR,\")\n",
    "    if U_Global:\n",
    "        output_file.write(\"U_GLOBAL,\")\n",
    "    if U_Global2:\n",
    "        output_file.write(\"U_GLOBAL,\")\n",
    "    if U_IR:\n",
    "        output_file.write(\"U_IR,\")\n",
    "    if Zenith:\n",
    "        output_file.write(\"Zenith\\n\")\n",
    "        \n",
    "### This is the function I use to actually write the headers into the CSV ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(input, 'r') as input_file:\n",
    "    with open(out_name, 'w') as output_file:\n",
    "        for count, line in enumerate(input_file):\n",
    "            if count < 4:\n",
    "                words = line.split()\n",
    "                for word in words:\n",
    "                    if word == \"DIRECT\":\n",
    "                        Direct = True\n",
    "                    if word == \"DIFFUSE\":\n",
    "                        Diffuse = True\n",
    "                    if word == \"DIFFUSE2\":\n",
    "                        Diffuse2 = True\n",
    "                    if word == \"D_GLOBAL\":\n",
    "                        D_Global = True\n",
    "                    if word == \"D_IR\":\n",
    "                        D_IR = True\n",
    "                    if word == \"U_GLOBAL\":\n",
    "                        U_Global = True\n",
    "                    if word ==\"U_GLOBAL2\":\n",
    "                        U_Global2 = True\n",
    "                    if word == \"U_IR\":\n",
    "                        U_IR = True\n",
    "                    if word == \"Zenith\":\n",
    "                        Zenith = True\n",
    "            else:\n",
    "                if check!=True:\n",
    "                    writeHeaders(output_file, Direct, Diffuse, Diffuse2, D_Global, D_IR, U_Global, U_Global2, U_IR, Zenith)\n",
    "                    check=True\n",
    "                outLine = \",\".join(line.split())\n",
    "                output_file.write(outLine + '\\n')\n",
    "                        \n",
    "input_file.close()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll try to summarize the above snippet:\n",
    "\n",
    "I open the .dat file and I create a .csv file to write to. I iterate through each line and check to see if I'm on a line before line 4 (python line reades are zero-based) because I know all the headers are in lines 0-3. The operation \"line.split()\" returns everything delimited by any whitespace, so this gets each word from the header. I then check for every header and set its flag to \"true.\" If I've gone through the header lines I then check to see if I've already written my headers (this is what the check variable looks for). If I haven't, I write them (essentially only headers that have been found will be written). Then I split the remaining data by whitespace, delimit it by commas, and write it out.\n",
    "\n",
    "With not much more work, you can use this script to iterate through all DAT files and write out accordingly. However, a CSV file is not our goal, NetCDF is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting CSV to NetCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick google search reveals that there are endless ways to do this, but I've decided to use xarray and pandas. My workflow was to make a dataframe from each CSV and then use that dataframe to export out to a NetCDF file. I chose this method because of how flexible a dataframe is. For instance, a dataframe could contain all the data from a testing location (even with the changing headers) or even all the readiation data in total. Here is what I did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from os.path import basename\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "NetCDF: Name contains illegal characters",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e8019d6e4da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"TestSite\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ALT\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mxds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mxds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mout_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hats/harvey/anaconda2/lib/python2.7/site-packages/xarray/core/dataset.pyc\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_netcdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         return to_netcdf(self, path, mode, format=format, group=group,\n\u001b[0;32m--> 782\u001b[0;31m                          engine=engine, encoding=encoding)\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0mdump\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_alias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dump'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hats/harvey/anaconda2/lib/python2.7/site-packages/xarray/backends/api.pyc\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path, mode, format, group, engine, writer, encoding)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_to_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hats/harvey/anaconda2/lib/python2.7/site-packages/xarray/core/dataset.pyc\u001b[0m in \u001b[0;36mdump_to_store\u001b[0;34m(self, store, encoder, sync, encoding)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hats/harvey/anaconda2/lib/python2.7/site-packages/xarray/backends/common.pyc\u001b[0m in \u001b[0;36mstore\u001b[0;34m(self, variables, attributes, check_encoding_set)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mcf_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcf_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcf_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         AbstractWritableDataStore.store(self, cf_variables, cf_attrs,\n\u001b[0;32m--> 234\u001b[0;31m                                         check_encoding_set)\n\u001b[0m",
      "\u001b[0;32m/Users/hats/harvey/anaconda2/lib/python2.7/site-packages/xarray/backends/common.pyc\u001b[0m in \u001b[0;36mstore\u001b[0;34m(self, variables, attributes, check_encoding_set)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                 if not (k in neccesary_dims and\n\u001b[1;32m    208\u001b[0m                                         is_trivial_index(v)))\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_encoding_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hats/harvey/anaconda2/lib/python2.7/site-packages/xarray/backends/common.pyc\u001b[0m in \u001b[0;36mset_variables\u001b[0;34m(self, variables, check_encoding_set)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode_variable_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheck_encoding_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hats/harvey/anaconda2/lib/python2.7/site-packages/xarray/backends/netCDF4_.pyc\u001b[0m in \u001b[0;36mprepare_variable\u001b[0;34m(self, name, variable, check_encoding)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mendian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'native'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mleast_significant_digit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'least_significant_digit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             fill_value=fill_value)\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0mnc4_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_auto_maskandscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.createVariable (netCDF4/_netCDF4.c:17091)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Variable.__init__ (netCDF4/_netCDF4.c:28572)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NetCDF: Name contains illegal characters"
     ]
    }
   ],
   "source": [
    "input = \"bao_1992_01.csv\"\n",
    "df1=pd.DataFrame()\n",
    "out_name = \"test.nc\"\n",
    "df1 = pd.read_csv(input,\n",
    "    sep=\",\",\n",
    "    parse_dates = {\"Date\":[0,1,2,3,4]},\n",
    "    date_parser=lambda x:pd.to_datetime(x,format=\"%Y %m %d %H %M\"),\n",
    "    index_col=['Date'])\n",
    "df1.loc[:,\"TestSite\"]=\"ALT\"\n",
    "xds=xr.Dataset.from_dataframe(df1)\n",
    "xds.to_netcdf(out_name)\n",
    "print out_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
